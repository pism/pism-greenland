import sys
import numpy as np
import os
import re
import pandas as pd
import pyproj
from uafgi import gdalutil,ogrutil,shputil
from uafgi import pdutil,shapelyutil
import shapely
import shapely.geometry
from osgeo import ogr,osr
import uafgi.data
import uafgi.data.bkm15
import uafgi.data.cf20
import uafgi.data.fj
import uafgi.data.m17
import uafgi.data.mwp
import uafgi.data.ns481
import uafgi.data.ns642
import uafgi.data.w21 as d_w21
import uafgi.data.wkt
from uafgi.data import greenland,stability
import pickle

"""Determine a set of glaciers for our experiment."""



# ------------------------------------------------------------------
def select_glaciers(includes):
    """w21_blackouts:
        DataFrame with index matching w21, cols do not matter; rows we do NOT want to select.
    select: filter1, all
        Apply filters to the set of Wood et al 2021 glaciers?
    """


    glaciers = w21.replace(df=glaciers.copy())


def select_glaciers_main():

    map_wkt = uafgi.data.wkt.nsidc_ps_north

    pd.set_option('display.max_columns', None)

    # Read user overrides of joins and columns
    over = stability.read_overrides()

    # Get initial list of glaciers
    # Master set of glaciers from which we will select
#    w21 = d_w21.read(uafgi.data.wkt.nsidc_ps_north)

    # Get a single terminus point for each glacier; our initial selection
    w21t = d_w21.read_termini(map_wkt)

    # ------------------- Remove blackout glaciers
    # Set up glacers we DON'T want to select
    w21t_blackouts = pd.DataFrame({
        'w21t_Glacier' : [
            # Kakivfaat glacier has one glacier front in Wood data, but two in NSIDC-0642 (as it separates)
            'Kakivfaat',
        ]
    })

    # Get the original w21.df's index onto the blackouts list
    blackouts_index= pd.merge(w21t.df.reset_index(), w21t_blackouts, how='inner', on='w21t_Glacier').set_index('index').index

    # Remove items we previously decided we DID NOT want to select
    w21t.df = w21t.df.drop(blackouts_index, axis=0)
    # ---------------------------------
    w21t.df = w21t.df[w21t.df.w21t_Glacier != 'Kakivfaat']
#    w21tx_termini = pdutil.group_and_tuplelist(w21t.df, 'w21t_Glacier', [ ('terminus_by_date', ['w21t_date', 'w21t_terminus']) ])
    w21tx = d_w21.termini_by_glacier(w21t)

    # Convert [(date, terminus), ...] into a MultiPoint object
    w21tx.df['w21t_points'] = w21tx.df['w21t_date_termini'].map(
            lambda date_terminus_list: shapelyutil.pointify(shape for _,shape in date_terminus_list))
    # Get the centroid of the multipoint; now it's some sort of point within the temrinus region
    w21tx.df['w21t_tloc'] = w21tx.df['w21t_points'].map(lambda mpt: mpt.centroid)

    # Join back with w21
    w21 = d_w21.read(map_wkt)
    w21tx.df = pdutil.merge_nodups(w21tx.df, w21.df, how='left', left_on='w21t_glacier_number', right_on='w21_glacier_number')
    # Now we have column w21_key

    # Identify the hand-drawn fjord matched to each glacier in our selection
    fj = uafgi.data.fj.read(uafgi.data.wkt.nsidc_ps_north)
    ret = uafgi.data.fj.match_to_points(
        w21tx, 'w21t_glacier_number', 'w21t_points',
        fj, debug_shapefile='fjdup.shp')
    select = ret['select']
    select.df = select.df.drop('w21t_points', axis=1)    # No longer need, it takes up space

    # PS: if there's a problem, ret['glaciersdup'] and ret['fjdup'] will be set; see 'fjdup.shp' file.
    select.df = select.df.dropna(subset=['fj_fid'])    # Drop glaciers without a hand-drawn fjord

    # Obtain set of local MEAUSRES grids on Greenland
    ns481 = uafgi.data.ns481.read(uafgi.data.wkt.nsidc_ps_north)

    match = pdutil.match_point_poly(select, 'w21t_tloc', ns481, 'ns481_poly',
        left_cols=['fj_poly'], right_cols=['ns481_poly'])
    match.df['fjord_grid_overlap'] = match.df.apply(
            lambda x: 0 if (type(x['ns481_poly'])==float or type(x['fj_poly']) == float)
            else x['ns481_poly'].intersection(x['fj_poly']).area / x['fj_poly'].area,
            axis=1)

#    match.df.sort_values(['w21t_ix'])

    try:
        select = match.left_join(overrides=over[['w21_key', 'ns481_key']])
    except pdutil.JoinError as err:
        print(err.df[['w21_key', 'ns481_key']].sort_values('ns481_key'))
        raise

    # Only keep glaciers inside a MEASURES grid
    select.df = select.df[~select.df['ns481_key'].isna()]

    # ----- Add a single upstream point for each glacier
    up = shputil.read_df(
        uafgi.data.join('upstream/upstream_points.shp'),
        wkt=uafgi.data.wkt.nsidc_ps_north, shape='loc', add_prefix='up_')

    match = pdutil.match_point_poly(up, 'up_loc', select, 'fj_poly').swap()

    try:
        select = match.left_join()
    except pdutil.JoinError as err:
        df = err.df[['w21t_key']].drop_duplicates()
        df = pdutil.merge_nodups(df, select.df[['w21t_key','fj_poly', 'fj_fid']], how='left', on='w21t_key')
        # Write fjords with duplicate upstream points to a shapefile
        fields=[ogr.FieldDefn('fj_fid',ogr.OFTInteger)]
        shputil.write_shapefile2(df['fj_poly'].tolist(), 'fjupdup.shp', fields, attrss=list(zip(df['fj_fid'].tolist())))

        # Print it out:
        print('Fjords with duplicate upstream points:\n{}'.format(df))

    # -------------------------------------------------------------
    # Get historical termini


    # ---- Join with CALFIN dataset high-frequency termini
    cf20= uafgi.data.cf20.read(uafgi.data.wkt.nsidc_ps_north)
#    cf20.df.to_csv('cf20.csv')
    print('===================================')
    match = pdutil.match_point_poly(cf20, 'cf20_locs', select, 'fj_poly').swap()
    try:
        select = match.left_join(overrides=over)
    except pdutil.JoinError as err:
        print(err.df)
        raise

    # ----- Join with NSIDC-0642 (MEASURES) annual termini
    ns642 = uafgi.data.ns642.read(uafgi.data.wkt.nsidc_ps_north)
    # Combine all points for each GlacierID
#    ns642x = uafgi.data.ns642.by_glacier_id(ns642)

    ns642x = uafgi.data.ns642.termini_by_glacier(ns642)

    # Convert [(date, terminus), ...] into a MultiPoint object
    ns642x.df['ns642_points'] = ns642x.df['ns642_date_termini'].map(
            lambda date_terminus_list: shapelyutil.pointify(shape for _,shape in date_terminus_list))

    print('******** ns642x {}'.format(ns642x.df.columns))

    match = pdutil.match_point_poly(
        ns642x, 'ns642_points', select, 'fj_poly').swap()
#    print(match.df)
#    match = match.df[match.df['w21_key'] != 'Kakivfaat']
#        right_cols=['lat','lon','w21_key']).swap()
#    print('xyz1', match.df.columns)
    select = match.left_join(overrides=over)

#    select.df = pdutil.merge_nodups(select.df, 
    return select



    # ----- Add termini from Wood et al 2021
    w21t = d_w21.read5_termini(uafgi.data.wkt.nsidc_ps_north)
    w21tp = d_w21.termini_by_glacier(w21t)
    match = pdutil.match_point_poly(w21tp, 'w21t_points', select, 'fj_poly').swap()
    select = match.left_join(overrides=over)
    select.df = select.df.drop(['w21t_points'], axis=1)
    # Don't do this yet, it repeats rows...
    # select.df = pdutil.merge_nodups(select.df, w21t.df, how='left', on='w21t_Glacier')
    
    selT,selF = pdutil.split_na(select.df, 'cf20_key')
    cols = ['w21_popular_name', 'w21_coast', 'ns481_grid', 'cf20_key', 'ns642_key']

    os.makedirs(uafgi.data.join_outputs('stability'), exist_ok=True)
    with open(uafgi.data.join_outputs('stability/01_select.dfx'), 'wb') as out:
        pickle.dump(select, out)
    select.df.to_pickle(uafgi.data.join_outputs('stability/01_select.df'))
    seldf = select.df.drop(['cf20_locs', 'ns642_points', 'ns481_poly'], axis=1)
    seldf.to_csv(uafgi.data.join_outputs('stability/01_select.csv'))


    return select

# ====================================================================




#select_glaciers_main()
